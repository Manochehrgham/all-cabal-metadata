homepage: https://github.com/bitemyapp/bloodhound
changelog-type: markdown
hash: 1b67af6b6da3bfef96fd0e6dd469ef61d954e0da6534480304b1a15e88a84909
test-bench-deps:
  http-client: -any
  bytestring: -any
  base: -any
  time: -any
  unordered-containers: ! '>=0.2.5.0 && <0.3'
  hspec: ! '>=1.8 && <2.3'
  text: -any
  filepath: -any
  doctest: ! '>=0.10.1'
  semigroups: -any
  containers: -any
  quickcheck-properties: -any
  bloodhound: -any
  mtl: -any
  errors: -any
  QuickCheck: -any
  doctest-prop: -any
  http-types: -any
  aeson: -any
  vector: -any
  derive: -any
  directory: -any
maintainer: cma@bitemyapp.com
synopsis: ElasticSearch client library for Haskell
changelog: ! "0.11.0.0\n===================\n\nThanks to the following people, Bloodhound
  0.10.0.0 is being released! This one gets a bit messy due to the Aeson 0.11 changeover,
  but this should be good to go now. Note that Aeson 0.11 returned to Aeson 0.9's
  behavior and semantics.\n\n* @MichaelXavier\n  - #112 List indices support\n  -
  #94 Implement index optimization\n  - #91 Make `respIsTwoHunna` more semantic\n
  \   - More detail: This is actually the cause of a bug in real code. If you happen
  to be\n      using parseEsResponse (which uses respIsTwoHunna) to parse the result
  of\n      certain operations such as creating an index, those operations return
  a\n      201 and unjustly are deemed to be a failure.\n  - Cleaned up errant Haskell
  tokens in README\n  - #84 Added request auth hooks\n\n* @dzhus / @MailOnline\n  -
  #85 Add updateDocument\n\n* @ReadmeCritic\n  - #108 Update README URLs based on
  HTTP redirects\n\n* @MHova\n  - #105 Add helper data types and functions for Missing
  Aggregations\n  - Removed unused server versions from the tests\n  - Updated readme
  to reflect actual ES versions supported and tested\n  - Added support for parsing
  results of Missing Aggregations\n  - #104 Export BucketValue\n  - #102 Add local
  testing instructions to the README\n  - #89 Support Bool and Numeric keys in TermsResults\n
  \ - Added Missing Aggregation support\n  - #98 Improve EsProtocolException documentation
  for human error\n  - Updated README to warn about 2.0 compatibility\n  - Fix docs
  specifying an incorrect terminating condition\n\n* @bitemyapp\n  - Merge monkey,
  puzzled over spurious local doctest failures\n\n0.10.0.0\n===================\n\nThanks
  to the following people, Bloodhound 0.10.0.0 is being released! This one gets a
  bit messy due to the Aeson 0.10 upgrade, so you may want to wait for the dust to
  settle. YMMV.\n\n* @MichaelXavier\n  - #77: Add test for error parsing\n  - #76/#78:
  Support for updating (modifying) index settings\n  - #79/#80: Index aliases\n  -
  #81: Low-level scroll API support\n  - #82: Date range aggregation\n\n* @bitemyapp\n
  \ - Fucked around with dependencies and broke things in order to upgrade to Aeson
  0.10\n  - Please forgive me.\n\n0.9.0.0\n===================\n\nThanks to the following
  people, Bloodhound 0.9.0.0 is being released!\n\n* @MichaelXavier\n  - #75: A more
  explicit type for errors\n  - #74: Add readme and changelog to extra source files\n\n*
  @MaxDaten\n  - #38/#73 Provide safety by using URL-encoding\n\n* @centromere\n  -
  #72 Added parent support to documentExists\n\n\n0.8.0.0\n===================\n\nThanks
  to the following people, Bloodhound 0.8.0.0 is being released!\n\n* @MichaelXavier\n
  \ - #67: Deriving Monad(Throw|Catch|Mask)\n  - #64: Export BH constructor\n  - #61:
  Filter aggregation support\n  - #60: Add value_count aggregation support\n  - #58:
  Eliminate partiality in EsResult\n  \n* @centromere\n  - #59: Fixed bug with IndexSettings
  serialization\n  - #56: Added fields support to Search\n  - #55: Added ability to
  specify a parent document\n  - #54: Fixed IndexTemplate serialization bug\n  - #52:
  Added ability to manipulate templates\n  - #51: Fixed mapping API\n  - #50: Fixed
  problem with put sending POST\n  \n* @bermanjosh\n  - #63: Url query encoding bug\n
  \ - #53: Scan type\n\n* @sjakobi\n  - #69: Replace Control.Monad.Error with CM.Except
  via mtl-compat\n  - #70: Silence redundant import warning with base-4.8.*\n  - #71:
  Use \"newManager\" instead of deprecated \"withManager\"\n\n0.7.0.0\n===================\n\n*
  Added QueryFilter thanks to Bjørn Nordbø!\n\n* Support for optimistic concurrency
  control thanks again to @MichaelXavier!\n\n0.6.0.1\n===================\n\n* Allow
  Aeson 0.9\n\n0.6.0.0\n===================\n\n* Moved to BHMonad, thanks to @MichaelXavier!
  Now there's a reader of config information and IO is lifted.\n\n* SearchHits have
  a Monoid now, makes combining search results nicer, allows for defaulting when a
  search cannot be performed.\n\n0.5.0.0\n===================\n\n* Fixed and changed
  TermsQuery (This caused the major bump)\n\n* Removed benchmarks from travis.yml\n\n*
  Added doctests, examples for Database.Bloodhound.Client. Haddocks should be much
  nicer.\n\n* Various fixes, reformatting\n\n0.4.0.0\n===================\n\n* Term
  and date aggregation - thanks to Christopher Guiney! (@chrisguiney)\n\nFollowing
  three thanks to Liam Atkins (@latkins)\n\n* omitNulls changed to exclude empty lists
  and null values\n\n* BoolQuery must/mustNot/Should changed from Maybe (Query|[Query])
  to [Query] thanks to @latkins\n\n* Added vector dependency so we can check for V.null/V.empty
  on JSON arrays\n\n* Highlighting, thanks to @latkins! See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html
  and http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/highlighting-intro.html
  for more\n\n* Added 1.4.0 support and CI integration\n\n* Can generate individual
  bulk operations, https://github.com/bitemyapp/bloodhound/issues/17, bulk requests
  should be more efficient now too - Vector instead of List.\n\n0.3.0.0\n===================\n\n*
  Status \"ok\" changed from Bool to Maybe Bool thanks to @borisyukd\n\n* Elasticsearch
  1.3.x compatibility fixed with changes to geo bounding boxes - thanks to Curtis
  Carter! (@ccarter)\n\n* CI coverage expanded to 1.0.x -> 1.3.x\n\n0.2.0.1\n===================\n\n*
  Killed off maybeJson/mField/catMaybes in favor of omitNulls\n\n* Experimenting with
  RecordWildcards\n\n* Merged Types and Instances module into Types to prevent possibility
  of orphans and cull orphan instance warnings.\n\n* Added note about current supported
  Elasticsearch version.\n\n0.2.0.0\n===================\n\n* Added TermFilter\n\n*
  Renamed createMapping to putMapping\n\n* Fixed and rebuilt documentation\n\n* RegexpFlags
  changed to a sum type instead of Text, thanks to @MichaelXavier!\n"
basic-deps:
  http-client: ! '>=0.5 && <0.6'
  mtl-compat: -any
  exceptions: -any
  bytestring: ! '>=0.10.0 && <0.11'
  base: ! '>=4.3 && <5'
  data-default-class: -any
  time: ! '>=1.4 && <1.7'
  unordered-containers: -any
  text: ! '>=0.11 && <1.3'
  semigroups: ! '>=0.15 && <0.19'
  containers: ! '>=0.5.0.0 && <0.6'
  blaze-builder: -any
  network-uri: ! '>=2.6 && <2.7'
  mtl: ! '>=1.0 && <2.3'
  hashable: -any
  transformers: ! '>=0.2 && <0.6'
  scientific: ! '>=0.3.0.0 && <0.4.0.0'
  http-types: ! '>=0.8 && <0.10'
  aeson: ! '>=0.11.1 && <0.12'
  vector: ! '>=0.10.9 && <0.12'
all-versions:
- '0.4.0.0'
- '0.4.0.1'
- '0.4.0.2'
- '0.5.0.0'
- '0.5.0.1'
- '0.6.0.0'
- '0.6.0.1'
- '0.7.0.0'
- '0.7.0.1'
- '0.8.0.0'
- '0.9.0.0'
- '0.10.0.0'
- '0.11.0.0'
- '0.11.1.0'
author: Chris Allen
latest: '0.11.1.0'
description-type: markdown
description: ! "Bloodhound [![TravisCI](https://travis-ci.org/bitemyapp/bloodhound.svg)](https://travis-ci.org/bitemyapp/bloodhound)
  [![Hackage](https://img.shields.io/hackage/v/bloodhound.svg?style=flat)](https://hackage.haskell.org/package/bloodhound)\n==========\n\n![Bloodhound
  (dog)](./bloodhound.jpg)\n\nElasticsearch client and query DSL for Haskell\n==============================================\n\nWhy?\n----\n\nSearch
  doesn't have to be hard. Let the dog do it.\n\nEndorsements\n------------\n\n\"Bloodhound
  makes Elasticsearch almost tolerable!\" - Almost-gruntled user\n\n\"ES is a nightmare
  but Bloodhound at least makes it tolerable.\" - Same user, later opinion.\n\nVersion
  compatibility\n---------------------\n\nElasticsearch \\>=1.0 && \\<2.0 is recommended.
  Bloodhound mostly works with 0.9.x, but I don't recommend it if you expect everything
  to work. As of Bloodhound 0.3 all \\>=1.0 && \\<2.0 versions of Elasticsearch work.
  Some (or even most?) features will work with versions \\>=2.0, but it is not officially
  supported yet.\n\nCurrent versions we test against are 1.2.4, 1.3.6, 1.4.1, 1.5.2,
  1.6.0, and 1.7.2. We also check that GHC 7.6 and 7.8 both build and pass tests.
  See our [TravisCI](https://travis-ci.org/bitemyapp/bloodhound) to learn more.\n\nStability\n---------\n\nBloodhound
  is stable for production use. I will strive to avoid breaking API compatibility
  from here on forward, but dramatic features like a type-safe, fully integrated mapping
  API may require breaking things in the future.\n\nTesting\n---------\n\nThe TravisCI
  tests are run using [Stack](http://docs.haskellstack.org/en/stable/README.html).
  You should use Stack instead of `cabal` to build and test Bloodhound to avoid compatibility
  problems. You will also need to have an ElasticSearch instance running at `localhost:9200`
  in order to execute some of the tests. See the \"Version compatibility\" section
  above for a list of ElasticSearch versions that are officially validated against
  in TravisCI.\n\nSteps to run the tests locally:\n  1. Dig through the [past releases]
  (https://www.elastic.co/downloads/past-releases) section of the ElasticSearch download
  page and install the desired ElasticSearch versions.\n  2. Install [Stack] (http://docs.haskellstack.org/en/stable/README.html#how-to-install)\n
  \ 3. In your local Bloodhound directory, run `stack setup && stack build`\n  4.
  Start the desired version of ElasticSearch at `localhost:9200`, which should be
  the default.\n  5. Run `stack test` in your local Bloodhound directory.\n  6. The
  unit tests will pass if you re-execute `stack test`, but some of the doctests might
  fail due to existing data in ElasticSearch. If you want to start with a clean slate,
  stop your ElasticSearch instance, delete the `data/` folder in the ElasticSearch
  installation, restart ElasticSearch, and re-run `stack test`.\n\n\nHackage page
  and Haddock documentation\n======================================\n\n<http://hackage.haskell.org/package/bloodhound>\n\nElasticsearch
  Tutorial\n======================\n\nIt's not using Bloodhound, but if you need an
  introduction to or overview of Elasticsearch and how to use it, you can use [this
  screencast](https://vimeo.com/106463167).\n\nExamples\n========\n\nIndex Operations\n----------------\n\n###
  Create Index\n\n``` {.haskell}\n\n-- Formatted for use in ghci, so there are \"let\"s
  in front of the decls.\n\n-- if you see :{ and :}, they're so you can copy-paste\n--
  the multi-line examples into your ghci REPL.\n\n:set -XDeriveGeneric\n:{\nimport
  Control.Applicative\nimport Database.Bloodhound\nimport Data.Aeson\nimport Data.Either
  (Either(..))\nimport Data.Maybe (fromJust)\nimport Data.Time.Calendar (Day(..))\nimport
  Data.Time.Clock (secondsToDiffTime, UTCTime(..))\nimport Data.Text (Text)\nimport
  GHC.Generics (Generic)\nimport Network.HTTP.Client\nimport qualified Network.HTTP.Types.Status
  as NHTS\n\n-- no trailing slashes in servers, library handles building the path.\nlet
  testServer = (Server \"http://localhost:9200\")\nlet testIndex = IndexName \"twitter\"\nlet
  testMapping = MappingName \"tweet\"\nlet withBH' = withBH defaultManagerSettings
  testServer\n\n-- defaultIndexSettings is exported by Database.Bloodhound as well\nlet
  defaultIndexSettings = IndexSettings (ShardCount 3) (ReplicaCount 2)\n\n-- createIndex
  returns MonadBH m => m Reply. You can use withBH for\n   one-off commands or you
  can use runBH to group together commands\n   and to pass in your own HTTP manager
  for pipelining.\n\n-- response :: Reply, Reply is a synonym for Network.HTTP.Conduit.Response\nresponse
  <- withBH' $ createIndex defaultIndexSettings testIndex\n:}\n\n```\n\n### Delete
  Index\n\n#### Code\n\n``` {.haskell}\n\n-- response :: Reply\nresponse <- withBH'
  $ deleteIndex testIndex\n\n```\n\n#### Example Response\n\n``` {.haskell}\n\n--
  print response if it was a success\nResponse {responseStatus = Status {statusCode
  = 200, statusMessage = \"OK\"}\n        , responseVersion = HTTP/1.1\n        ,
  responseHeaders = [(\"Content-Type\", \"application/json; charset=UTF-8\")\n                           ,
  (\"Content-Length\", \"21\")]\n        , responseBody = \"{\\\"acknowledged\\\":true}\"\n
  \       , responseCookieJar = CJ {expose = []}\n        , responseClose' = ResponseClose}\n\n--
  if the index to be deleted didn't exist anyway\nResponse {responseStatus = Status
  {statusCode = 404, statusMessage = \"Not Found\"}\n        , responseVersion = HTTP/1.1\n
  \       , responseHeaders = [(\"Content-Type\", \"application/json; charset=UTF-8\")\n
  \                          , (\"Content-Length\",\"65\")]\n        , responseBody
  = \"{\\\"error\\\":\\\"IndexMissingException[[twitter] missing]\\\",\\\"status\\\":404}\"\n
  \       , responseCookieJar = CJ {expose = []}\n        , responseClose' = ResponseClose}\n\n```\n\n###
  Refresh Index\n\n#### Note, you **have** to do this if you expect to read what you
  just wrote\n\n``` {.haskell}\n\nresp <- withBH' $ refreshIndex testIndex\n\n```\n\n####
  Example Response\n\n``` {.haskell}\n\n-- print resp on success\nResponse {responseStatus
  = Status {statusCode = 200, statusMessage = \"OK\"}\n        , responseVersion =
  HTTP/1.1\n        , responseHeaders = [(\"Content-Type\", \"application/json; charset=UTF-8\")\n
  \                          , (\"Content-Length\",\"50\")]\n        , responseBody
  = \"{\\\"_shards\\\":{\\\"total\\\":10,\\\"successful\\\":5,\\\"failed\\\":0}}\"\n
  \       , responseCookieJar = CJ {expose = []}\n        , responseClose' = ResponseClose}\n\n```\n\nMapping
  Operations\n------------------\n\n### Create Mapping\n\n``` {.haskell}\n\n-- don't
  forget imports and the like at the top.\n\ndata TweetMapping = TweetMapping deriving
  (Eq, Show)\n\n-- I know writing the JSON manually sucks.\n-- I don't have a proper
  data type for Mappings yet.\n-- Let me know if this is something you need.\n\n:{\ninstance
  ToJSON TweetMapping where\n  toJSON TweetMapping =\n    object [\"tweet\" .=\n      object
  [\"properties\" .=\n        object [\"location\" .=\n          object [\"type\"
  .= (\"geo_point\" :: Text)]]]]\n:}\n\nresp <- withBH' $ putMapping testIndex testMapping
  TweetMapping\n\n```\n\n### Delete Mapping\n\n``` {.haskell}\n\nresp <- withBH' $
  deleteMapping testIndex testMapping\n\n```\n\nDocument Operations\n-------------------\n\n###
  Indexing Documents\n\n``` {.haskell}\n\n-- don't forget the imports and derive generic
  setting for ghci\n-- at the beginning of the examples.\n\n:{\ndata Location = Location
  { lat :: Double\n                         , lon :: Double } deriving (Eq, Generic,
  Show)\n\ndata Tweet = Tweet { user     :: Text\n                   , postDate ::
  UTCTime\n                   , message  :: Text\n                   , age      ::
  Int\n                   , location :: Location } deriving (Eq, Generic, Show)\n\nexampleTweet
  = Tweet { user     = \"bitemyapp\"\n                     , postDate = UTCTime\n
  \                                 (ModifiedJulianDay 55000)\n                                  (secondsToDiffTime
  10)\n                     , message  = \"Use haskell!\"\n                     ,
  age      = 10000\n                     , location = Location 40.12 (-71.34) }\n\n--
  automagic (generic) derivation of instances because we're lazy.\ninstance ToJSON
  \  Tweet\ninstance FromJSON Tweet\ninstance ToJSON   Location\ninstance FromJSON
  Location\n:}\n\n-- Should be able to toJSON and encode the data structures like
  this:\n-- λ> toJSON $ Location 10.0 10.0\n-- Object fromList [(\"lat\",Number 10.0),(\"lon\",Number
  10.0)]\n-- λ> encode $ Location 10.0 10.0\n-- \"{\\\"lat\\\":10,\\\"lon\\\":10}\"\n\nresp
  <- withBH' $ indexDocument testIndex testMapping defaultIndexDocumentSettings exampleTweet
  (DocId \"1\")\n\n```\n\n#### Example Response\n\n``` {.haskell}\n\nResponse {responseStatus
  =\n  Status {statusCode = 200, statusMessage = \"OK\"}\n    , responseVersion =
  HTTP/1.1, responseHeaders =\n    [(\"Content-Type\",\"application/json; charset=UTF-8\"),\n
  \    (\"Content-Length\",\"75\")]\n    , responseBody = \"{\\\"_index\\\":\\\"twitter\\\",\\\"_type\\\":\\\"tweet\\\",\\\"_id\\\":\\\"1\\\",\\\"_version\\\":2,\\\"created\\\":false}\"\n
  \   , responseCookieJar = CJ {expose = []}, responseClose' = ResponseClose}\n\n```\n\n###
  Deleting Documents\n\n``` {.haskell}\n\nresp <- withBH' $ deleteDocument testIndex
  testMapping (DocId \"1\")\n\n```\n\n### Getting Documents\n\n``` {.haskell}\n\n--
  n.b., you'll need the earlier imports. responseBody is from http-conduit\n\nresp
  <- withBH' $ getDocument testIndex testMapping (DocId \"1\")\n\n-- responseBody
  :: Response body -> body\nlet body = responseBody resp\n\n-- you have two options,
  you use decode and just get Maybe (EsResult Tweet)\n-- or you can use eitherDecode
  and get Either String (EsResult Tweet)\n\nlet maybeResult = decode body :: Maybe
  (EsResult Tweet)\n-- the explicit typing is so Aeson knows how to parse the JSON.\n\n--
  use either if you want to know why something failed to parse.\n-- (string errors,
  sadly)\nlet eitherResult = eitherDecode body :: Either String (EsResult Tweet)\n\n--
  print eitherResult should look like:\nRight (EsResult {_index = \"twitter\"\n               ,
  _type = \"tweet\"\n               , _id = \"1\"\n               , foundResult =
  Just (EsResultFound { _version = 2\n                                                   ,
  _source = Tweet {user = \"bitemyapp\"\n                                                                     ,
  postDate = 2009-06-18 00:00:10 UTC\n                                                                     ,
  message = \"Use haskell!\"\n                                                                     ,
  age = 10000\n                                                                     ,
  location = Location {lat = 40.12, lon = -71.34}}})})\n\n-- _source in EsResultFound
  is parametric, we dispatch the type by passing in what we expect (Tweet) as a parameter
  to EsResult.\n\n-- use the _source record accessor to get at your document\nfmap
  (fmap _source . foundResult) eitherResult\nRight (Just (Tweet {user = \"bitemyapp\"\n
  \                  , postDate = 2009-06-18 00:00:10 UTC\n                   , message
  = \"Use haskell!\"\n                   , age = 10000\n                   , location
  = Location {lat = 40.12, lon = -71.34}}))\n\n```\n\nBulk Operations\n---------------\n\n###
  Bulk create, index\n\n``` {.haskell}\n\n-- don't forget the imports and derive generic
  setting for ghci\n-- at the beginning of the examples.\n\n:{\n-- Using the earlier
  Tweet datatype and exampleTweet data\n\n-- just changing up the data a bit.\nlet
  bulkTest = exampleTweet { user = \"blah\" }\nlet bulkTestTwo = exampleTweet { message
  = \"woohoo!\" }\n\n-- create only bulk operation\n-- BulkCreate :: IndexName ->
  MappingName -> DocId -> Value -> BulkOperation\nlet firstOp = BulkCreate testIndex\n
  \             testMapping (DocId \"3\") (toJSON bulkTest)\n\n-- index operation
  \"create or update\"\nlet sndOp   = BulkIndex testIndex\n              testMapping
  (DocId \"4\") (toJSON bulkTestTwo)\n\n-- Some explanation, the final \"Value\" type
  that BulkIndex,\n-- BulkCreate, and BulkUpdate accept is the actual document\n--
  data that your operation applies to. BulkDelete doesn't\n-- take a value because
  it's just deleting whatever DocId\n-- you pass.\n\n-- list of bulk operations\nlet
  stream = [firstDoc, secondDoc]\n\n-- Fire off the actual bulk request\n-- bulk ::
  Vector BulkOperation -> IO Reply\nresp <- withBH' $ bulk stream\n:}\n\n```\n\n###
  Encoding individual bulk API operations\n\n``` {.haskell}\n-- the following functions
  are exported in Bloodhound so\n-- you can build up bulk operations yourself\nencodeBulkOperations
  :: V.Vector BulkOperation -> L.ByteString\nencodeBulkOperation :: BulkOperation
  -> L.ByteString\n\n-- How to use the above:\ndata BulkTest = BulkTest { name ::
  Text } deriving (Eq, Generic, Show)\ninstance FromJSON BulkTest\ninstance ToJSON
  BulkTest\n\n_ <- insertData\nlet firstTest = BulkTest \"blah\"\nlet secondTest =
  BulkTest \"bloo\"\nlet firstDoc = BulkIndex testIndex\n               testMapping
  (DocId \"2\") (toJSON firstTest)\nlet secondDoc = BulkCreate testIndex\n               testMapping
  (DocId \"3\") (toJSON secondTest)\nlet stream = V.fromList [firstDoc, secondDoc]
  :: V.Vector BulkOperation\n\n-- to encode yourself\nlet firstDocEncoded = encode
  firstDoc :: L.ByteString\n\n-- to encode a vector of bulk operations\nlet encodedOperations
  = encodeBulkOperations stream\n\n-- to insert into a particular server\n-- bulk
  :: V.Vector BulkOperation -> IO Reply\n_ <- withBH' $ bulk streamp\n\n```\n\nSearch\n------\n\n###
  Querying\n\n#### Term Query\n\n``` {.haskell}\n\n-- exported by the Client module,
  just defaults some stuff.\n-- mkSearch :: Maybe Query -> Maybe Filter -> Search\n--
  mkSearch query filter = Search query filter Nothing False (From 0) (Size 10) Nothing\n\nlet
  query = TermQuery (Term \"user\" \"bitemyapp\") Nothing\n\n-- AND'ing identity filter
  with itself and then tacking it onto a query\n-- search should be a null-operation.
  I include it for the sake of example.\n-- <||> (or/plus) should make it into a search
  that returns everything.\n\nlet filter = IdentityFilter <&&> IdentityFilter\n\n--
  constructing the search object the searchByIndex function dispatches on.\nlet search
  = mkSearch (Just query) (Just filter)\n\n-- you can also searchByType and specify
  the mapping name.\nreply <- withBH' $ searchByIndex testIndex search\n\nlet result
  = eitherDecode (responseBody reply) :: Either String (SearchResult Tweet)\n\nλ>
  fmap (hits . searchHits) result\nRight [Hit {hitIndex = IndexName \"twitter\"\n
  \         , hitType = MappingName \"tweet\"\n          , hitDocId = DocId \"1\"\n
  \         , hitScore = 0.30685282\n          , hitSource = Tweet {user = \"bitemyapp\"\n
  \                            , postDate = 2009-06-18 00:00:10 UTC\n                             ,
  message = \"Use haskell!\"\n                             , age = 10000\n                             ,
  location = Location {lat = 40.12, lon = -71.34}}}]\n\n```\n\n#### Match Query\n\n```
  {.haskell}\n\nlet query = QueryMatchQuery $ mkMatchQuery (FieldName \"user\") (QueryString
  \"bitemyapp\")\nlet search = mkSearch (Just query) Nothing\n\n```\n\n#### Multi-Match
  Query\n\n``` {.haskell}\n\nlet fields = [FieldName \"user\", FieldName \"message\"]\nlet
  query = QueryMultiMatchQuery $ mkMultiMatchQuery fields (QueryString \"bitemyapp\")\nlet
  search = mkSearch (Just query) Nothing\n\n```\n\n#### Bool Query\n\n``` {.haskell}\n\nlet
  innerQuery = QueryMatchQuery $\n                 mkMatchQuery (FieldName \"user\")
  (QueryString \"bitemyapp\")\nlet query = QueryBoolQuery $\n            mkBoolQuery
  [innerQuery] [] []\nlet search = mkSearch (Just query) Nothing\n\n```\n\n#### Boosting
  Query\n\n``` {.haskell}\n\nlet posQuery = QueryMatchQuery $\n               mkMatchQuery
  (FieldName \"user\") (QueryString \"bitemyapp\")\nlet negQuery = QueryMatchQuery
  $\n               mkMatchQuery (FieldName \"user\") (QueryString \"notmyapp\")\nlet
  query = QueryBoostingQuery $\n            BoostingQuery posQuery negQuery (Boost
  0.2)\n\n```\n\n#### Rest of the query/filter types\n\nJust follow the pattern you've
  seen here and check the Hackage API documentation.\n\n### Sorting\n\n``` {.haskell}\n\nlet
  sortSpec = DefaultSortSpec $ mkSort (FieldName \"age\") Ascending\n\n-- mkSort is
  a shortcut function that takes a FieldName and a SortOrder\n-- to generate a vanilla
  DefaultSort.\n-- checkt the DefaultSort type for the full list of customizable options.\n\n--
  From and size are integers for pagination.\n\n-- When sorting on a field, scores
  are not computed. By setting TrackSortScores to true, scores will still be computed
  and tracked.\n\n-- type Sort = [SortSpec]\n-- type TrackSortScores = Bool\n-- type
  From = Int\n-- type Size = Int\n\n-- Search takes Maybe Query\n--              ->
  Maybe Filter\n--              -> Maybe Sort\n--              -> TrackSortScores\n--
  \             -> From -> Size\n--              -> Maybe [FieldName]\n\n-- just add
  more sortspecs to the list if you want tie-breakers.\nlet search = Search Nothing
  (Just IdentityFilter) (Just [sortSpec]) False (From 0) (Size 10) Nothing\n\n```\n\n###
  Field selection\n\nIf you only want certain fields from the source document returned,
  you can\nset the \"fields\" field of the Search record.\n\n``` {.haskell}\n\nlet
  search' = mkSearch (Just (MatchAllQuery Nothing)) Nothing\n    search  = search'
  { fields = Just [FieldName \"updated\"] }\n\n```\n\n### Filtering\n\n#### And, Not,
  and Or filters\n\nFilters form a monoid and seminearring.\n\n``` {.haskell}\n\ninstance
  Monoid Filter where\n  mempty = IdentityFilter\n  mappend a b = AndFilter [a, b]
  defaultCache\n\ninstance Seminearring Filter where\n  a <||> b = OrFilter [a, b]
  defaultCache\n\n-- AndFilter and OrFilter take [Filter] as an argument.\n\n-- This
  will return anything, because IdentityFilter returns everything\nOrFilter [IdentityFilter,
  someOtherFilter] False\n\n-- This will return exactly what someOtherFilter returns\nAndFilter
  [IdentityFilter, someOtherFilter] False\n\n-- Thanks to the seminearring and monoid,
  the above can be expressed as:\n\n-- \"and\"\nIdentityFilter <&&> someOtherFilter\n\n--
  \"or\"\nIdentityFilter <||> someOtherFilter\n\n-- Also there is a NotFilter, it
  only accepts a single filter, not a list.\n\nNotFilter someOtherFilter False\n\n```\n\n####
  Identity Filter\n\n``` {.haskell}\n\n-- And'ing two Identity\nlet queryFilter =
  IdentityFilter <&&> IdentityFilter\n\nlet search = mkSearch Nothing (Just queryFilter)\n\nreply
  <- withBH' $ searchByType testIndex testMapping search\n\n```\n\n#### Boolean Filter\n\nSimilar
  to boolean queries.\n\n``` {.haskell}\n\n-- Will return only items whose \"user\"
  field contains the term \"bitemyapp\"\nlet queryFilter = BoolFilter (MustMatch (Term
  \"user\" \"bitemyapp\") False)\n\n-- Will return only items whose \"user\" field
  does not contain the term \"bitemyapp\"\nlet queryFilter = BoolFilter (MustNotMatch
  (Term \"user\" \"bitemyapp\") False)\n\n-- The clause (query) should appear in the
  matching document.\n-- In a boolean query with no must clauses, one or more should\n--
  clauses must match a document. The minimum number of should\n-- clauses to match
  can be set using the minimum_should_match parameter.\nlet queryFilter = BoolFilter
  (ShouldMatch [(Term \"user\" \"bitemyapp\")] False)\n\n```\n\n#### Exists Filter\n\n```
  {.haskell}\n\n-- Will filter for documents that have the field \"user\"\nlet existsFilter
  = ExistsFilter (FieldName \"user\")\n\n```\n\n#### Geo BoundingBox Filter\n\n```
  {.haskell}\n\n-- topLeft and bottomRight\nlet box = GeoBoundingBox (LatLon 40.73
  (-74.1)) (LatLon 40.10 (-71.12))\n\nlet constraint = GeoBoundingBoxConstraint (FieldName
  \"tweet.location\") box False GeoFilterMemory\n\n```\n\n#### Geo Distance Filter\n\n```
  {.haskell}\n\nlet geoPoint = GeoPoint (FieldName \"tweet.location\") (LatLon 40.12
  (-71.34))\n\n-- coefficient and units\nlet distance = Distance 10.0 Miles\n\n--
  GeoFilterType or NoOptimizeBbox\nlet optimizeBbox = OptimizeGeoFilterType GeoFilterMemory\n\n--
  SloppyArc is the usual/default optimization in Elasticsearch today\n-- but pre-1.0
  versions will need to pick Arc or Plane.\n\nlet geoFilter = GeoDistanceFilter geoPoint
  distance SloppyArc optimizeBbox False\n\n```\n\n#### Geo Distance Range Filter\n\nThink
  of a donut and you won't be far off.\n\n``` {.haskell}\n\nlet geoPoint = GeoPoint
  (FieldName \"tweet.location\") (LatLon 40.12 (-71.34))\n\nlet distanceRange = DistanceRange
  (Distance 0.0 Miles) (Distance 10.0 Miles)\n\nlet geoFilter = GeoDistanceRangeFilter
  geoPoint distanceRange\n\n```\n\n#### Geo Polygon Filter\n\n``` {.haskell}\n\n--
  I think I drew a square here.\nlet points = [LatLon 40.0 (-70.00),\n              LatLon
  40.0 (-72.00),\n              LatLon 41.0 (-70.00),\n              LatLon 41.0 (-72.00)]\n\nlet
  geoFilter = GeoPolygonFilter (FieldName \"tweet.location\") points\n\n```\n\n####
  Document IDs filter\n\n``` {.haskell}\n\n-- takes a mapping name and a list of DocIds\nIdsFilter
  (MappingName \"tweet\") [DocId \"1\"]\n\n```\n\n#### Range Filter\n\n``` {.haskell}\n\n--
  RangeFilter :: FieldName\n--                -> RangeValue\n--                ->
  RangeExecution\n--                -> Cache -> Filter\n\nlet filter = RangeFilter
  (FieldName \"age\")\n             (RangeGtLt (GreaterThan 1000.0) (LessThan 100000.0))\n
  \            RangeExecutionIndex False\n\n```\n\n``` {.haskell}\n\nlet filter =
  RangeFilter (FieldName \"age\")\n             (RangeLte (LessThanEq 100000.0))\n
  \            RangeExecutionIndex False\n\n```\n\n##### Date Ranges\n\nDate ranges
  are expressed in UTCTime. Date ranges use the same range bound constructors as numerics,
  except that they end in \"D\".\n\nNote that compatibility with ES is tested only
  down to seconds.\n\n``` {.haskell}\n\nlet filter = RangeFilter (FieldName \"postDate\")\n
  \            (RangeDateGtLte\n              (GreaterThanD (UTCTime\n                          (ModifiedJulianDay
  55000)\n                          (secondsToDiffTime 9)))\n              (LessThanEqD
  (UTCTime\n                            (ModifiedJulianDay 55000)\n                            (secondsToDiffTime
  11))))\n             RangeExecutionIndex False\n```\n\n#### Regexp Filter\n\n```
  {.haskell}\n\n-- RegexpFilter\n--   :: FieldName\n--      -> Regexp\n--      ->
  RegexpFlags\n--      -> CacheName\n--      -> Cache\n--      -> CacheKey\n--      ->
  Filter\nlet filter = RegexpFilter (FieldName \"user\") (Regexp \"bite.*app\")\n
  \            AllRegexpFlags (CacheName \"test\") False (CacheKey \"key\")\n\n--
  n.b.\n-- data RegexpFlags = AllRegexpFlags\n--                 | NoRegexpFlags\n--
  \                | SomeRegexpFlags (NonEmpty RegexpFlag) deriving (Eq, Show)\n\n--
  data RegexpFlag = AnyString\n--                | Automaton\n--                |
  Complement\n--                | Empty\n--                | Intersection\n--                |
  Interval deriving (Eq, Show)\n\n```\n\n### Aggregations\n\n#### Adding aggregations
  to search\n\nAggregations can now be added to search queries, or made on their own.\n\n```
  {.haskell}\ntype Aggregations = M.Map Text Aggregation\ndata Aggregation\n  = TermsAgg
  TermsAggregation\n  | DateHistogramAgg DateHistogramAggregation\n```\n\nFor convenience,
  \\`\\`\\`mkAggregations\\`\\`\\` exists, that will create an \\`\\`\\`Aggregations\\`\\`\\`
  with the aggregation provided.\n\nFor example:\n\n``` {.haskell}\n let a = mkAggregations
  \"users\" $ TermsAgg $ mkTermsAggregation \"user\"\n let search = mkAggregateSearch
  Nothing a\n```\n\nAggregations can be added to an existing search, using the \\`\\`\\`aggBody\\`\\`\\`
  field\n\n``` {.haskell}\n let search  = mkSearch (Just (MatchAllQuery Nothing))
  Nothing\n let search' = search {aggBody = Just a}\n```\n\nSince the \\`\\`\\`Aggregations\\`\\`\\`
  structure is just a Map Text Aggregation, M.insert can be used to add additional
  aggregations.\n\n``` {.haskell}\n let a' = M.insert \"age\" (TermsAgg $ mkTermsAggregation
  \"age\") a\n```\n\n#### Extracting aggregations from results\n\nAggregations are
  part of the reply structure of every search, in the\nform of `Maybe AggregationResults`\n\n```
  {.haskell}\n-- Lift decode and response body to be in the IO monad.\nlet decode'
  = liftM decode\nlet responseBody' = liftM responseBody\nlet reply = withBH' $ searchByIndex
  testIndex search\nlet response = decode' $ responseBody' reply :: IO (Maybe (SearchResult
  Tweet))\n\n-- Now that we have our response, we can extract our terms aggregation
  result -- which is a list of buckets.\n\nlet terms = do { response' <- response;
  return $ response' >>= aggregations >>= toTerms \"users\" }\nterms\nJust (Bucket
  {buckets = [TermsResult {termKey = \"bitemyapp\", termsDocCount = 1, termsAggs =
  Nothing}]})\n```\n\nNote that bucket aggregation results, such as the TermsResult
  is a member of the type class `BucketAggregation`:\n\n``` {.haskell}\nclass BucketAggregation
  a where\n  key :: a -> Text\n  docCount :: a -> Int\n  aggs :: a -> Maybe AggregationResults\n```\n\nYou
  can use the `aggs` function to get any nested results, if there\nwere any. For example,
  if there were a nested terms aggregation keyed\nto \"age\" in a TermsResult named
  `termresult` , you would call `aggs\ntermresult >>= toTerms \"age\"`\n\n#### Terms
  Aggregation\n\n``` {.haskell}\ndata TermsAggregation\n  = TermsAggregation {term
  :: Either Text Text,\n                      termInclude :: Maybe TermInclusion,\n
  \                     termExclude :: Maybe TermInclusion,\n                      termOrder
  :: Maybe TermOrder,\n                      termMinDocCount :: Maybe Int,\n                      termSize
  :: Maybe Int,\n                      termShardSize :: Maybe Int,\n                      termCollectMode
  :: Maybe CollectionMode,\n                      termExecutionHint :: Maybe ExecutionHint,\n
  \                     termAggs :: Maybe Aggregations}\n```\n\nTerm Aggregations
  have two factory functions, `mkTermsAggregation`, and\n`mkTermsScriptAggregation`,
  and can be used as follows:\n\n``` {.haskell}\nlet ta = TermsAgg $ mkTermsAggregation
  \"user\"\n```\n\nThere are of course other options that can be added to a Terms
  Aggregation, such as the collection mode:\n\n``` {.haskell}\nlet ta   = mkTermsAggregation
  \"user\"\nlet ta'  = ta { termCollectMode = Just BreadthFirst }\nlet ta'' = TermsAgg
  ta'\n```\n\nFor more documentation on how the Terms Aggregation works, see <https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html>\n\n####
  Date Histogram Aggregation\n\n``` {.haskell}\ndata DateHistogramAggregation\n  =
  DateHistogramAggregation {dateField :: FieldName,\n                              dateInterval
  :: Interval,\n                              dateFormat :: Maybe Text,\n                              datePreZone
  :: Maybe Text,\n                              datePostZone :: Maybe Text,\n                              datePreOffset
  :: Maybe Text,\n                              datePostOffset :: Maybe Text,\n                              dateAggs
  :: Maybe Aggregations}\n```\n\nThe Date Histogram Aggregation works much the same
  as the Terms Aggregation.\n\nRelevant functions include `mkDateHistogram`, and `toDateHistogram`\n\n```
  {.haskell}\nlet dh = DateHistogramAgg (mkDateHistogram (FieldName \"postDate\")
  Minute)\n```\n\nDate histograms also accept a `FractionalInterval`:\n\n``` {.haskell}\nFractionalInterval
  :: Float -> TimeInterval -> Interval\n-- TimeInterval is the following:\ndata TimeInterval
  = Weeks | Days | Hours | Minutes | Seconds\n```\n\nIt can be used as follows:\n\n```
  {.haskell}\nlet dh = DateHistogramAgg (mkDateHistogram (FieldName \"postDate\")
  (FractionalInterval 1.5 Minutes))\n```\n\nThe `DateHistogramResult` is defined as:\n\n```
  {.haskell}\ndata DateHistogramResult\n  = DateHistogramResult {dateKey :: Int,\n
  \                        dateKeyStr :: Maybe Text,\n                         dateDocCount
  :: Int,\n                         dateHistogramAggs :: Maybe AggregationResults}\n```\n\nIt
  is an instance of `BucketAggregation`, and can have nested aggregations in each
  bucket.\n\nBuckets can be extracted from an `AggregationResult` using\n`toDateHistogram
  name`\n\nFor more information on the Date Histogram Aggregation, see: <https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html>\n\n\nContributors\n============\n\n*
  [Chris Allen](https://github.com/bitemyapp)\n* [Liam Atkinson](https://github.com/latkins)\n*
  [Christopher Guiney](https://github.com/chrisguiney)\n* [Curtis Carter](https://github.com/ccarter)\n*
  [Michael Xavier](https://github.com/MichaelXavier)\n* [Bob Long](https://github.com/bobjflong)\n*
  [Maximilian Tagher](https://github.com/MaxGabriel)\n* [Anna Kopp](https://github.com/annakopp)\n*
  [Matvey B. Aksenov](https://github.com/supki)\n* [Jan-Philip Loos](https://github.com/MaxDaten)\n\nPossible
  future functionality\n=============================\n\nSpan Queries\n------------\n\nBeginning
  here: <https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-span-first-query.html>\n\nFunction
  Score Query\n--------------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html>\n\nNode
  discovery and failover\n---------------------------\n\nMight require TCP support.\n\nSupport
  for TCP access to Elasticsearch\n---------------------------------------\n\nPretend
  to be a transport client?\n\nBulk cluster-join merge\n-----------------------\n\nMight
  require making a lucene index on disk with the appropriate format.\n\nGeoShapeQuery\n-------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-geo-shape-query.html>\n\nGeoShapeFilter\n--------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-geo-shape-filter.html>\n\nGeohash
  cell filter\n-------------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-geohash-cell-filter.html>\n\nHasChild
  Filter\n---------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-has-child-filter.html>\n\nHasParent
  Filter\n----------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-has-parent-filter.html>\n\nIndices
  Filter\n--------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-indices-filter.html>\n\nQuery
  Filter\n------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-query-filter.html>\n\nScript
  based sorting\n--------------------\n\n<https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html#_script_based_sorting>\n\nCollapsing
  redundantly nested and/or structures\n-----------------------------------------------\n\nThe
  Seminearring instance, if deeply nested can possibly produce nested structure that
  is redundant. Depending on how this affects ES performance, reducing this structure
  might be valuable.\n\nRuntime checking for cycles in data structures\n----------------------------------------------\n\ncheck
  for n \\> 1 occurrences in DFS:\n\n<http://hackage.haskell.org/package/stable-maps-0.0.5/docs/System-Mem-StableName-Dynamic.html>\n\n<http://hackage.haskell.org/package/stable-maps-0.0.5/docs/System-Mem-StableName-Dynamic-Map.html>\n\nPhoto
  Origin\n============\n\nPhoto from HA! Designs: <https://www.flickr.com/photos/hadesigns/>\n"
license-name: BSD3

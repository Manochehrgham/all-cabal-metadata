homepage: https://github.com/ndmitchell/tagsoup#readme
changelog-type: text
hash: 6bc0644c5397c40cb2df411c0089ed6e858f108431a5cda4bac5823b92de9b5f
test-bench-deps: {}
maintainer: Neil Mitchell <ndmitchell@gmail.com>
synopsis: Parsing and extracting information from (possibly malformed) HTML/XML documents
changelog: ! "Changelog for TagSoup\n\n0.13.7\n    #32, make sure upper case &#X works
  in lookupEntity\n0.13.6\n    #28, some named entities require a trailing semicolon
  (e.g. mid)\n0.13.5\n    #26, rename the test program to test-tagsoup\n0.13.4\n    #24,
  add isTagComment function\n    Update the copyright year\n0.13.3\n    Work on GHC
  7.9\n0.13.2\n    Remove all package upper bounds\n    Allow QuickCheck-2.6\n0.13.1\n
  \   #562, treat <script> specially as per HTML5\n0.13\n    #616, extend to all HTML5
  entities\n    Optimise lookupNamedEntity\n    Replace escapeXMLChar with escapeXML\n
  \   Change all Entity functions to return String, not Int or Char\n0.12.7\n    Allow
  deepseq-1.3\n    Allow QuickCheck-2.5\n    Support bytestring-0.10 (NFData instances
  added)\n0.12.6\n    #515, don't crash on malformed characters (use ? instead)\n0.12.5\n
  \   Add optRawTag to Render, to ensure script's don't get escaped\n0.12.4\n    #487,
  fix the behaviour of ~== for TagComment and others\n0.12.3\n    GHC 7.2 compatibility\n0.12.2\n
  \   Add StringLike instances for Text\n0.12.1\n    Add parseOptionsEntities and
  improve documentation\n0.12\n    Upgrade to QuickCheck 2.4.*\n    Export toTagRep\n
  \   Make the -download flag off by default\n    Eliminate HTTP dependency\n    Eliminate
  mtl dependency\n0.11.1\n    Support --flags=-download to eliminate the network dependency\n0.11\n
  \   #326, <a \"foo\"> is no longer treated as an attribute\n    Add Eq/Ord instances
  to Tree\n    Don't mark Text.HTML.TagSoup.Tree as preliminary\n    #325, \\r should
  be treated as whitespace\n0.10.1\n    #322, don't change ' to &apos; in render (do
  what the docs say)\n0.10\n    Improve the cabal file, make the test program off
  by default\n    Expose Text.HTML.TagSoup.Match again (was hidden accidentally)\n0.9\n
  \   #294, let optEntityData see if there was a ';' (CHANGES API)\n    Numeric/hex
  entities in attributes were misparsed\n    #149, add escapeHTML function\n0.8\n
  \   Parser now based on HTML5 specification\n    Tag is now parameterised by the
  string type\n0.6\n    Addition of Text.HTML.TagSoup.Tree and Text.HTML.TagSoup.Render\n
  \   Text.HTML.TagSoup.Parser.Options renamed to ParseOptions\n    Text.HTML.TagSoup.Parser.options
  renamed to parseOptions\n0.4\n    Changelog started\n"
basic-deps:
  bytestring: -any
  base: ==4.*
  text: -any
  containers: -any
all-versions:
- '0.1'
- '0.4'
- '0.6'
- '0.8'
- '0.9'
- '0.10'
- '0.10.1'
- '0.11'
- '0.11.1'
- '0.12'
- '0.12.1'
- '0.12.2'
- '0.12.3'
- '0.12.4'
- '0.12.5'
- '0.12.6'
- '0.12.7'
- '0.12.8'
- '0.13'
- '0.13.1'
- '0.13.2'
- '0.13.3'
- '0.13.4'
- '0.13.5'
- '0.13.6'
- '0.13.7'
author: Neil Mitchell <ndmitchell@gmail.com>
latest: '0.13.7'
description-type: markdown
description: ! "# TagSoup [![Hackage version](https://img.shields.io/hackage/v/tagsoup.svg?label=Hackage)](https://hackage.haskell.org/package/tagsoup)
  [![Stackage version](https://www.stackage.org/package/tagsoup/badge/lts?label=Stackage)](https://www.stackage.org/package/tagsoup)
  [![Linux Build Status](https://img.shields.io/travis/ndmitchell/tagsoup.svg?label=Linux%20build)](https://travis-ci.org/ndmitchell/tagsoup)
  [![Windows Build Status](https://img.shields.io/appveyor/ci/ndmitchell/tagsoup.svg?label=Windows%20build)](https://ci.appveyor.com/project/ndmitchell/tagsoup)\n\nTagSoup
  is a library for parsing HTML/XML. It supports the HTML 5 specification, and can
  be used to parse either well-formed XML, or unstructured and malformed HTML from
  the web. The library also provides useful functions to extract information from
  an HTML document, making it ideal for screen-scraping.\n\nThe library provides a
  basic data type for a list of unstructured tags, a parser to convert HTML into this
  tag type, and useful functions and combinators for finding and extracting information.
  This document gives two particular examples of scraping information from the web,
  while a few more may be found in the [Sample](https://github.com/ndmitchell/tagsoup/blob/master/TagSoup/Sample.hs)
  file from the source repository. The examples we give are:\n\n* Obtaining the Hit
  Count from Haskell.org\n* Obtaining a list of Simon Peyton-Jones' latest papers\n*
  A brief overview of some other examples\n\nThe intial version of this library was
  written in Javascript and has been used for various commercial projects involving
  screen scraping. In the examples general hints on screen scraping are included,
  learnt from bitter experience. It should be noted that if you depend on data which
  someone else may change at any given time, you may be in for a shock!\n\nThis library
  was written without knowledge of the Java version of [TagSoup](http://home.ccil.org/~cowan/XML/tagsoup/).
  They have made a very different design decision: to ensure default attributes are
  present and to properly nest parsed tags. We do not do this - tags are merely a
  list devoid of nesting information.\n\n\n#### Acknowledgements\n\nThanks to Mike
  Dodds for persuading me to write this up as a library. Thanks to many people for
  debugging and code contributions, including: Gleb Alexeev, Ketil Malde, Conrad Parker,
  Henning Thielemann, Dino Morelli, Emily Mitchell, Gwern Branwen.\n\n\n## Potential
  Bugs\n\nThere are two things that may go wrong with these examples:\n\n* _The Websites
  being scraped may change._ There is nothing I can do about this, but if you suspect
  this is the case let me know, and I'll update the examples and tutorials. I have
  already done so several times, its only a few minutes work.\n* _The `openURL` method
  may not work._ This happens quite regularly, and depending on your server, proxies
  and direction of the wind, they may not work. The solution is to use `wget` to download
  the page locally, then use `readFile` instead. Hopefully a decent Haskell HTTP library
  will emerge, and that can be used instead.\n\n\n## Haskell Hit Count\n\nOur goal
  is to develop a program that displays the Haskell.org hit count. This example covers
  all the basics in designing a basic web-scraping application.\n\n### Finding the
  Page\n\nWe first need to find where the information is displayed, and in what format.
  Taking a look at the [front web page](http://www.haskell.org/haskellwiki/Haskell),
  when not logged in, we see:\n\n    <ul id=\"f-list\">\n        <li id=\"lastmod\">
  This page was last modified on 9 September 2013, at 22:38.</li>\n        <li id=\"viewcount\">This
  page has been accessed 6,985,922 times.</li>\n        <li id=\"copyright\">Recent
  content is available under <a href=\"/haskellwiki/HaskellWiki:Copyrights\" title=\"HaskellWiki:Copyrights\">a
  simple permissive license</a>.</li>\n        <li id=\"privacy\"><a href=\"/haskellwiki/HaskellWiki:Privacy_policy\"
  title=\"HaskellWiki:Privacy policy\">Privacy policy</a></li>\n        <li id=\"about\"><a
  href=\"/haskellwiki/HaskellWiki:About\" title=\"HaskellWiki:About\">About HaskellWiki</a></li>\n
  \       <li id=\"disclaimer\"><a href=\"/haskellwiki/HaskellWiki:General_disclaimer\"
  title=\"HaskellWiki:General disclaimer\">Disclaimers</a></li>\n    </ul>\n\nSo we
  see the hit count is available. This leads us to rule 1:\n\n**Rule 1:** Scrape from
  what the page returns, not what a browser renders, or what view-source gives.\n\nSome
  web servers will serve different content depending on the user agent, some browsers
  will have scripting modify their displayed HTML, some pages will display differently
  depending on your cookies. Before you can start to figure out how to start scraping,
  first decide what the input to your program will be. There are two ways to get the
  page as it will appear to your program.\n\n#### Using the HTTP package\n\nWe can
  write a simple HTTP downloader with using the [HTTP package](http://hackage.haskell.org/package/HTTP):\n\n
  \   import Network.HTTP\n    \n    openURL x = getResponseBody =<< simpleHTTP (getRequest
  x)\n    \n    main = do src <- openURL \"http://www.haskell.org/haskellwiki/Haskell\"\n
  \             writeFile \"temp.htm\" src\n\nNow open `temp.htm`, find the fragment
  of HTML containing the hit count, and examine it.\n\n#### Using the `tagsoup` Program\n\nTagsoup
  installs both as a library and a program. The program contains all the examples
  mentioned on this page, along with a few other useful functions. In order to download
  a URL to a file:\n\n    $ tagsoup grab http://www.haskell.org/haskellwiki/Haskell
  > temp.htm\n\n### Finding the Information\n\nNow we examine both the fragment that
  contains our snippet of information, and the wider page. What does the fragment
  has that nothing else has? What algorithm would we use to obtain that particular
  element? How can we still return the element as the content changes? What if the
  design changes? But wait, before going any further:\n\n**Rule 2:** Do not be robust
  to design changes, do not even consider the possibility when writing the code.\n\nIf
  the user changes their website, they will do so in unpredictable ways. They may
  move the page, they may put the information somewhere else, they may remove the
  information entirely. If you want something robust talk to the site owner, or buy
  the data from someone. If you try and think about design changes, you will complicate
  your design, and it still won't work. It is better to write an extraction method
  quickly, and happily rewrite it when things change.\n\nSo now, lets consider the
  fragment from above. It is useful to find a tag which is unique just above your
  snippet - something with a nice \"id\" property, or a \"class\" - something which
  is unlikely to occur multiple times. In the above example, \"viewcount\" as the
  id seems perfect.\n\n    haskellHitCount = do\n        src <- openURL \"http://haskell.org/haskellwiki/Haskell\"\n
  \       let count = fromFooter $ parseTags src\n        putStrLn $ \"haskell.org
  has been hit \" ++ count ++ \" times\"\n        where fromFooter = filter isDigit
  . innerText . take 2 . dropWhile (~/= \"<li id=viewcount>\")\n\nNow we start writing
  the code! The first thing to do is open the required URL, then we parse the code
  into a list of `Tag`s with `parseTags`. The `fromFooter` function does the interesting
  thing, and can be read right to left:\n\n* First we throw away everything (`dropWhile`)
  until we get to an `li` tag containing `id=viewcount`. The `(~==)` operator is different
  from standard equality, allowing additional attributes to be present. We write `\"<li
  id=viewcount>\"` as syntactic sugar for `TagOpen \"li\" [(\"id\",\"viewcount\")]`.
  If we just wanted any open tag with the given id we could have written `(~== TagOpen
  \"\" [(\"id\",\"viewcount\")])` and this would have matched. Any empty strings in
  the second element of the match are considered as wildcards.\n* Next we take two
  elements, the `<li>` tag and the text node immediately following.\n* We call the
  `innerText` function to get all the text values from inside, which will just be
  the text node following the `viewcount`.\n* We keep only the numbers, getting rid
  of the surrounding text and the commas.\n\nThis code may seem slightly messy, and
  indeed it is - often that is the nature of extracting information from a tag soup.\n\n**Rule
  3:** TagSoup is for extracting information where structure has been lost, use more
  structured information if it is available.\n\n\n## Simon's Papers\n\nOur next very
  important task is to extract a list of all Simon Peyton Jones' recent research papers
  off his [home page](http://research.microsoft.com/en-us/people/simonpj/). The largest
  change to the previous example is that now we desire a list of papers, rather than
  just a single result.\n\nAs before we first start by writing a simple program that
  downloads the appropriate page, and look for common patterns. This time we want
  to look for all patterns which occur every time a paper is mentioned, but no where
  else. The other difference from last time is that previous we grabbed an automatically
  generated piece of information - this time the information is entered in a more
  freeform way by a human.\n\nFirst we spot that the page helpfully has named anchors,
  there is a current work anchor, and after that is one for Haskell. We can extract
  all the information between them with a simple `take`/`drop` pair:\n\n    takeWhile
  (~/= \"<a name=haskell>\") $\n    drop 5 $ dropWhile (~/= \"<a name=current>\")
  tags\n\nThis code drops until you get to the \"current\" section, then takes until
  you get to the \"haskell\" section, ensuring we only look at the important bit of
  the page. Next we want to find all hyperlinks within this section:\n\n    map f
  $ sections (~== \"<A>\") $ ...\n\nRemember that the function to select all tags
  with name \"A\" could have been written as `(~== TagOpen \"A\" [])`, or alternatively
  `isTagOpenName \"A\"`. Afterwards we map each item with an `f` function. This function
  needs to take the tags starting just after the link, and find the text inside the
  link.\n\n    f = dequote . unwords . words . fromTagText . head . filter isTagText\n\nHere
  the complexity of interfacing to human written markup comes through. Some of the
  links are in italic, some are not - the `filter` drops all those that are not, until
  we find a pure text node. The `unwords . words` deletes all multiple spaces, replaces
  tabs and newlines with spaces and trims the front and back - a neat trick when dealing
  with text which has spacing at the source code but not when displayed. The final
  thing to take account of is that some papers are given with quotes around the name,
  some are not - dequote will remove the quotes if they exist.\n\nFor completeness,
  we now present the entire example:\n    \n    spjPapers :: IO ()\n    spjPapers
  = do\n            tags <- fmap parseTags $ openURL \"http://research.microsoft.com/en-us/people/simonpj/\"\n
  \           let links = map f $ sections (~== \"<A>\") $\n                        takeWhile
  (~/= \"<a name=haskell>\") $\n                        drop 5 $ dropWhile (~/= \"<a
  name=current>\") tags\n            putStr $ unlines links\n        where\n            f
  :: [Tag] -> String\n            f = dequote . unwords . words . fromTagText . head
  . filter isTagText\n    \n            dequote ('\\\"':xs) | last xs == '\\\"' =
  init xs\n            dequote x = x\n\n## Other Examples\n\nSeveral more examples
  are given in the Example file, including obtaining the (short) list of papers from
  my site, getting the current time and a basic XML validator. All can be invoked
  using the `tagsoup` executable program. All use very much the same style as presented
  here - writing screen scrapers follow a standard pattern. We present the code from
  two for enjoyment only.\n\n### My Papers\n\n    ndmPapers :: IO ()\n    ndmPapers
  = do\n            tags <- fmap parseTags $ openURL \"http://community.haskell.org/~ndm/downloads/\"\n
  \           let papers = map f $ sections (~== \"<li class=paper>\") tags\n            putStr
  $ unlines papers\n        where\n            f :: [Tag] -> String\n            f
  xs = fromTagText (xs !! 2)\n\n### UK Time\n\n    currentTime :: IO ()\n    currentTime
  = do\n        tags <- fmap parseTags $ openURL \"http://www.timeanddate.com/worldclock/city.html?n=136\"\n
  \       let time = fromTagText (dropWhile (~/= \"<strong id=ct>\") tags !! 1)\n
  \       putStrLn time\n        \n<h2>Related Projects</h2>\n\n<ul>\n    <li><a href=\"http://tagsoup.info/\">TagSoup
  for Java</a> - an independently written malformed HTML parser for Java. Including
  <a href=\"http://tagsoup.info/#other\">links to other</a> HTML parsers.</li>\n    <li><a
  href=\"http://www.fh-wedel.de/~si/HXmlToolbox/\">HXT: Haskell XML Toolbox</a> -
  a more comprehensive XML parser, giving the option of using TagSoup as a lexer.</li>\n
  \   <li><a href=\"http://www.fh-wedel.de/~si/HXmlToolbox/#rel\">Other Related Work</a>
  - as described on the HXT pages.</li>\n    <li><a href=\"http://therning.org/magnus/archives/367\">Using
  TagSoup with Parsec</a> - a nice combination of Haskell libraries.</li>\n    <li><a
  href=\"http://hackage.haskell.org/packages/tagsoup-parsec\">tagsoup-parsec</a> -
  a library for easily using TagSoup as a token type in Parsec.</li>\n    <li><a href=\"http://hackage.haskell.org/packages/archive/wraxml/latest/doc/html/Text-XML-WraXML-Tree-TagSoup.html\">WraXML</a>
  - construct a lazy tree from TagSoup lexemes.</li>\n</ul>\n"
license-name: BSD3

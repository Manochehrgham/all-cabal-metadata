changelog-type: ''
hash: b2a98e2314e2c752029340ffbc5813ccaca5f601db10bf100df29a82d02d84a5
synopsis: Archive supplied URLs in WebCite & Internet Archive
changelog: ''
all-versions:
- '0.1'
- '0.2'
- '0.3'
- '0.3.1'
- '0.4'
- '0.5'
- '0.5.1'
- '0.6.0'
- '0.6.1'
- '0.6.2'
- '0.6.2.1'
latest: '0.6.2.1'
description-type: haddock
description: ! '`archiver` is a daemon which will process a specified text file,

  each line of which is a URL, and will (randomly) one by one request that

  the URLs be archived or spidered by <http://www.webcitation.org>,

  <http://www.archive.org>, and <http://www.wikiwix.com> for future reference.

  (One may optionally specify an arbitrary `sh` command like `wget` to download URLs
  locally.)


  Because the interface is a simple text file, this can be combined

  with other scripts; for example, a script using Sqlite to extract

  visited URLs from Firefox, or a program extracting URLs from Pandoc

  documents. (See <http://www.gwern.net/Archiving%20URLs>.)


  For explanation of the derivation of the code in `Network.URL.Archiver`,

  see <http://www.gwern.net/haskell/Wikipedia%20Archive%20Bot>.'

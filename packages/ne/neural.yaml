homepage: https://github.com/brunjlar/neural
changelog-type: ''
hash: 8f14022f2958e35b54a68d73a4d10981ef39ab34b2cf8ed072cc41fa498b515a
test-bench-deps:
  MonadRandom: ! '>=0.4.2.2 && <0.5'
  base: ! '>=4.7 && <5'
  hspec: ! '>=2.2.2 && <2.3'
  criterion: ! '>=1.1.1.0 && <1.2'
  doctest: ! '>=0.10.1 && <0.11'
  neural: ! '>=0.2.0.0 && <0.3'
  Glob: ! '>=0.7.5 && <0.8'
maintainer: brunjlar@gmail.com
synopsis: Neural Networks in native Haskell
changelog: ''
basic-deps:
  JuicyPixels: ! '>=3.2.7 && <3.3'
  bytestring: ! '>=0.10.6.0 && <0.11'
  monad-par: ! '>=0.3.4.7 && <0.4'
  reflection: ! '>=2.1.2 && <2.2'
  MonadRandom: ! '>=0.4.2.2 && <0.5'
  base: ! '>=4.7 && <5'
  pipes-bytestring: ! '>=2.1.1 && <2.2'
  ad: ! '>=4.3.2 && <4.4'
  hspec: ! '>=2.2.2 && <2.3'
  text: ! '>=1.2.2.1 && <1.3'
  typelits-witnesses: ! '>=0.2.0.0 && <0.3'
  kan-extensions: ! '>=4.2.3 && <4.3'
  monad-par-extras: ! '>=0.3.3 && <0.4'
  filepath: ! '>=1.4.0.0 && <1.5'
  array: ! '>=0.5.1.0 && <0.6'
  ghc-typelits-natnormalise: ! '>=0.4.1 && <0.5'
  neural: ! '>=0.2.0.0 && <0.3'
  STMonadTrans: ! '>=0.3.3 && <0.4'
  lens: ==4.13.*
  pipes: ! '>=4.1.8 && <4.2'
  pipes-zlib: ! '>=0.4.4 && <0.5'
  mtl: ! '>=2.2.1 && <2.3'
  attoparsec: ! '>=0.13.0.1 && <0.14'
  transformers: ! '>=0.4.2.0 && <0.5'
  parallel: ! '>=3.2.1.0 && <3.3'
  deepseq: ! '>=1.4.1.1 && <1.5'
  pipes-safe: ! '>=2.2.3 && <2.3'
  profunctors: ==5.2.*
  vector: ! '>=0.11.0.0 && <0.12'
  directory: ! '>=1.2.2.0 && <1.3'
all-versions:
- '0.1.0.0'
- '0.1.0.1'
- '0.1.1.0'
- '0.2.0.0'
author: Lars Bruenjes
latest: '0.2.0.0'
description-type: markdown
description: ! "# neural - Neural Nets in native Haskell\r\n\r\n[![Build Status](https://travis-ci.org/brunjlar/neural.svg?branch=master)](https://travis-ci.org/brunjlar/neural)\r\n\r\n##
  Motivation\r\n\r\nThe goal of this project is to provide a flexible framework for
  \r\n[neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) \r\n(and
  similar parameterized models) in Haskell.\r\n\r\nThere are already a couple of neural
  network libraries out there on Hackage, but as far as I can tell,\r\nthey either\r\n\r\n-
  are wrappers for an engine written in another language or\r\n- offer a limitted
  choice of network architectures, training algorithms or error functions\r\n  or
  are not easily extensible.\r\n\r\nThe goal of this library is to have an implementation
  in native Haskell (reasonably efficient)\r\nwhich offers maximal flexibility.\r\n\r\nFurthermore,
  [gradient descent/backpropagation](https://en.wikipedia.org/wiki/Backpropagation)
  should work automatically, using\r\n[automatic differentiation](https://hackage.haskell.org/package/ad-4.3.2/docs/Numeric-AD.html).\r\nThis
  means that new and complicated activation functions and/or network architectures
  can be used without the need\r\nto first calculate derivatives by hand.\r\n\r\nIn
  order to provide a powerful and flexible API, models are constructed using *components*
  which behave as if they implemented the\r\n[Arrow and ArrowChoice](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Arrow.html)
  typeclasses. \r\nThey can therefore easily be combined and transformed.\r\n\r\nOnce
  a model has been constructed, it can be hooked up into a customized training algorithm
  using [pipes](https://hackage.haskell.org/package/pipes),\r\nso that various aspects
  of the algorithm (loading data, choosing random samples, reporting intermediate
  results, stop criterium etc.) can be defined in a modular,\r\ndecoupled way.\r\n\r\nEven
  though neural networks are the primary motivation for this project, any other kind
  of model can be\r\ndefined in the same framework, whenever the model depends on
  a collection of numerical parameters in a differentiable\r\nway. - One simple example
  for this would be [linear regression](https://en.wikipedia.org/wiki/Linear_regression).\r\n\r\n##
  Examples\r\n\r\nAt the moment, three examples are included:\r\n\r\n- [sqrt](examples/sqrt)
  models the regression problem of approximating the square root function on the interval
  [0,4].\r\n\r\n- [iris](examples/iris) solves the famous [Iris Flower](https://en.wikipedia.org/wiki/Iris_flower_data_set)
  classification problem.\r\n\r\n- [MNIST](examples/MNIST) tackles the equally famous
  [MNIST](https://en.wikipedia.org/wiki/MNIST_database) problem of recognizing handwritten
  digits.\r\n"
license-name: MIT

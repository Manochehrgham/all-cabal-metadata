homepage: https://github.com/brunjlar/neural
changelog-type: ''
hash: af8cafd2e7f809f04ca0c32b6737176867567ecdbfcecda073528452b700c56b
test-bench-deps:
  MonadRandom: ! '>=0.4.2.2 && <0.5'
  base: ! '>=4.7 && <5'
  hspec: ! '>=2.2.2 && <2.3'
  doctest: ! '>=0.10.1 && <0.11'
  neural: ! '>=0.1.1.0 && <0.2'
  Glob: ! '>=0.7.5 && <0.8'
maintainer: brunjlar@gmail.com
synopsis: Neural Networks in native Haskell
changelog: ''
basic-deps:
  MonadRandom: ! '>=0.4.2.2 && <0.5'
  base: ! '>=4.7 && <5'
  ad: ! '>=4.3.2 && <4.4'
  hspec: ! '>=2.2.2 && <2.3'
  text: ! '>=1.2.2.1 && <1.3'
  typelits-witnesses: ! '>=0.2.0.0 && <0.3'
  filepath: ! '>=1.4.0.0 && <1.5'
  array: ! '>=0.5.1.0 && <0.6'
  ghc-typelits-natnormalise: ! '>=0.4.1 && <0.5'
  neural: ! '>=0.1.1.0 && <0.2'
  STMonadTrans: ! '>=0.3.3 && <0.4'
  lens: ==4.13.*
  pipes: ! '>=4.1.8 && <4.2'
  mtl: ! '>=2.2.1 && <2.3'
  attoparsec: ! '>=0.13.0.1 && <0.14'
  transformers: ! '>=0.4.2.0 && <0.5'
  parallel: ! '>=3.2.1.0 && <3.3'
  deepseq: ! '>=1.4.1.1 && <1.5'
  profunctors: ==5.2.*
  vector: ! '>=0.11.0.0 && <0.12'
  directory: ! '>=1.2.2.0 && <1.3'
all-versions:
- '0.1.0.0'
- '0.1.0.1'
- '0.1.1.0'
author: Lars Bruenjes
latest: '0.1.1.0'
description-type: markdown
description: ! "# neural - Neural Nets in native Haskell\r\n\r\n[![Build Status](https://travis-ci.org/brunjlar/neural.svg?branch=master)](https://travis-ci.org/brunjlar/neural)\r\n\r\nThe
  goal of this project is to provide a flexible framework for \r\n[neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network)
  \r\n(and similar parameterized models) in Haskell.\r\n\r\nThere are already a couple
  of neural network libraries out there on Hackage, but as far as I can tell,\r\nthey
  either\r\n\r\n- are wrappers for an engine written in another language or\r\n- offer
  a limitted choice of network architectures, training algorithms or error functions\r\n
  \ or are not easily extensible.\r\n\r\nThe goal of this library is to have an implementation
  in native Haskell (reasonably efficient)\r\nwhich offers maximal flexibility.\r\n\r\nFurthermore,
  [gradient descent/backpropagation](https://en.wikipedia.org/wiki/Backpropagation)
  should work automatically, using\r\n[automatic differentiation](https://hackage.haskell.org/package/ad-4.3.2/docs/Numeric-AD.html).\r\nThis
  means that new and complicated activation functions and/or network architectures
  can be used without the need\r\nto first calculate derivatives by hand.\r\n\r\nIn
  order to provide a powerful and flexible API, models are constructed using *components*
  which implement the\r\n[Arrow and ArrowChoice](https://hackage.haskell.org/package/base-4.9.0.0/docs/Control-Arrow.html)
  typeclasses. \r\nThey can therefore easily be combined and transformed, using a
  multitude of\r\navailable combinators or [arrow notation](http://downloads.haskell.org/~ghc/8.0.1/docs/html/users_guide/glasgow_exts.html#arrow-notation).\r\n\r\nEven
  though neural networks are the primary motivation for this project, any other kind
  of model can be\r\ndefined in the same framework, whenever the model depends on
  a collection of numerical parameters in a differentiable\r\nway. - One simple example
  for this would be [linear regression](https://en.wikipedia.org/wiki/Linear_regression).\r\n"
license-name: MIT

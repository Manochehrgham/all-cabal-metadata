homepage: https://github.com/mstksg/backprop
changelog-type: markdown
hash: a683b78a52dd257c0500f5699ee4036f0c101ad85216f9ecce39140d75084aab
test-bench-deps: {}
maintainer: justin@jle.im
synopsis: Heterogeneous, type-safe automatic backpropagation in Haskell
changelog: ! "Changelog\n=========\n\nVersion 0.0.1.0\n---------------\n\n<https://github.com/mstksg/uncertain/releases/tag/v0.0.1.0>\n\n*
  \  Initial pre-release, as a request for comments.  API is in a usable form\n    and
  everything is fully documented, but there are definitely some things\n    left to
  be done. (See [README.md][readme-0.0.1.0])\n\n    [readme-0.0.1.0]: https://github.com/mstksg/backprop/tree/v0.0.1.0#readme\n\n"
basic-deps:
  microlens-th: -any
  mwc-random: -any
  reflection: -any
  type-combinators: -any
  split: -any
  base: ! '>=4.7 && <5'
  time: -any
  ad: -any
  tagged: -any
  singletons: -any
  bifunctors: -any
  mtl: -any
  mnist-idx: -any
  transformers-base: -any
  transformers: -any
  deepseq: -any
  generics-sop: -any
  hmatrix: ! '>=0.18'
  finite-typelits: -any
  backprop: -any
  microlens-mtl: -any
  microlens: -any
  primitive: -any
  profunctors: -any
  vector: -any
all-versions:
- '0.0.1.0'
author: Justin Le
latest: '0.0.1.0'
description-type: markdown
description: ! "backprop\n========\n\n[![Build Status](https://travis-ci.org/mstksg/backprop.svg?branch=master)](https://travis-ci.org/mstksg/backprop)\n\n[**Literate
  Haskell Tutorial/Demo on MNIST data set**][mnist-lhs] (and [PDF\nrendering][mnist-pdf])\n\nAutomatic
  *heterogeneous* back-propagation that can be used either *implicitly*\n(in the style
  of the [ad][] library) or using *explicit* graphs built in\nmonadic style.  Implements
  reverse-mode automatic differentiation.  Differs\nfrom [ad][] by offering full heterogeneity
  -- each intermediate step and the\nresulting value can have different types.  Mostly
  intended for usage with\ntensor manipulation libraries to implement automatic back-propagation
  for\ngradient descent and other optimization techniques.\n\n[ad]: http://hackage.haskell.org/package/ad\n\nDocumentation
  is currently rendered [on github pages][docs]!\n\n[docs]: https://mstksg.github.io/backprop\n\nMNIST
  Digit Classifier Example\n------------------------------\n\nTutorial and example
  on training on the MNIST data set [available here as a\nliterate haskell file][mnist-lhs],
  or [rendered here as a PDF][mnist-pdf]!\n**Read this first!**\n\n[mnist-lhs]: https://github.com/mstksg/backprop/blob/master/samples/MNIST.lhs\n[mnist-pdf]:
  https://github.com/mstksg/backprop/blob/master/renders/MNIST.pdf\n\n\nBrief example\n-------------\n\nThe
  quick example below describes the running of a neural network with one\nhidden layer
  to calculate its squared error with respect to target `targ`,\nwhich is parameterized
  by two weight matrices and two bias vectors.\nVector/matrix types are from the *hmatrix*
  package.\n\n~~~haskell\nlogistic :: Floating a => a -> a\nlogistic x = 1 / (1 +
  exp (-x))\n\nmatVec\n    :: (KnownNat m, KnownNat n)\n    => Op '[ L m n, R n ]
  (R m)\n\nneuralNetImplicit\n      :: (KnownNat m, KnownNat n, KnownNat o)\n      =>
  R m\n      -> BPOpI s '[ L n m, R n, L o n, R o ] (R o)\nneuralNetImplicit inp =
  \\(w1 :< b1 :< w2 :< b2 :< Ø) ->\n    let z = logistic (liftB2 matVec w1 x + b1)\n
  \   in  logistic (liftB2 matVec w2 z + b2)\n  where\n    x = constRef inp\n\nneuralNetExplicit\n
  \     :: (KnownNat m, KnownNat n, KnownNat o)\n      => R m\n      -> BPOp s '[
  L n m, R n, L o n, R o ] (R o)\nneuralNetExplicit inp = withInps $ \\(w1 :< b1 :<
  w2 :< b2 :< Ø) -> do\n    y1  <- matVec ~$ (w1 :< x1 :< Ø)\n    let x2 = logistic
  (y1 + b1)\n    y2  <- matVec ~$ (w2 :< x2 :< Ø)\n    return $ logistic (y2 + b2)\n
  \ where\n    x1 = constVar inp\n~~~\n\nNow `neuralNetExplicit` and `neuralNetImplicit`
  can be \"run\" with the input\nvectors and parameters (a `L n m`, `R n`, `L o n`,
  and `R o`) and calculate the\noutput of the neural net.\n\n~~~haskell\nrunNet\n
  \   :: (KnownNat m, KnownNat n, KnownNat o)\n    => R m\n    -> Tuple '[ L n m,
  R n, L o n, R o ]\n    -> R o\nrunNet inp = evalBPOp (neuralNetExplicit inp)\n~~~\n\nBut,
  in defining `neuralNet`, we also generated a graph that *backprop* can\nuse to do
  back-propagation, too!\n\n~~~haskell\ndot :: KnownNat n\n    => Op '[ R n  , R n
  ] Double\n\nnetGrad\n    :: forall m n o. (KnownNat m, KnownNat n, KnownNat o)\n
  \   => R m\n    -> R o\n    -> Tuple '[ L n m, R n, L o n, R o ]\n    -> Tuple '[
  L n m, R n, L o n, R o ]\nnetGrad inp targ params = gradBPOp opError params\n  where\n
  \   -- calculate squared error, in *explicit* style\n    opError :: BPOp s '[ L
  n m, R n, L o n, R o ] Double\n    opError = do\n        res <- neuralNetExplicit
  inp\n        err <- bindRef (res - t)\n        dot ~$ (err :< err :< Ø)\n      where\n
  \       t = constRef targ\n~~~\n\nThe result is the gradient of the input tuple's
  components, with respect\nto the `Double` result of `opError` (the squared error).
  \ We can then use\nthis gradient to do gradient descent.\n\nFor a more fleshed out
  example, see the [MNIST tutorial][mnist-lhs] (also\n[rendered as a pdf][mnist-pdf])\n\nTodo\n----\n\n1.
  \ Actual profiling and benchmarking, to gauge how much overhead this library\n    adds
  over \"manual\" back-propagation.\n\n    Ideally this can be brought down to 0?\n\n2.
  \ Some simple performance and API tweaks that are probably possible now and\n    would
  clearly benefit: (if you want to contribute)\n\n    a.  Providing optimized `Num`/`Fractional`/`Floating`
  instances for `BVal`\n        by supplying known gradients directly instead of relying
  on *ad*.\n\n    b.  Switch from `ST s` to `IO`, and use `unsafePerformIO` to automatically\n
  \       bind `BVal`s (like *ad* does) when using `liftB`.  This might remove\n        some
  overhead during graph building, and, from an API standpoint,\n        remove the
  need for explicit binding.\n\n    c.  Switch from `STRef`s/`IORef`s to `Array`.
  \ (This one I'm unclear if it\n        would help any)\n\n3.  Benchmark against
  competing back-propagation libraries like *ad*, and\n    auto-differentiating tensor
  libraries like *[grenade][]*\n\n    [grenade]: https://github.com/HuwCampbell/grenade\n\n4.
  \ Explore opportunities for parallelization.  There are some naive ways of\n    directly
  parallelizing right now, but potential overhead should be\n    investigated.\n\n5.
  \ Some open questions:\n\n    a.  Is it possible to offer pattern matching on sum
  types/with different\n        constructors for implicit-graph backprop?  It's possible
  for\n        explicit-graph versions already, with `choicesVar`, but not yet with\n
  \       the implicit-graph interface.  Could be similar to an \"Applicative vs.\n
  \       Monad\" issue where you can only have pre-determined fixed computation\n
  \       paths when using `Applicative`, but I'm not sure.  Still, it would be\n
  \       nice, because if this was possible, we could possibly do away with\n        explicit-graph
  mode completely.\n\n    b.  Though we already have sum type support with explicit-graph
  mode, we\n        can't support GADTs yet.  It'd be nice to see if this is possible,\n
  \       because a lot of dependently typed neural network stuff is made much\n        simpler
  with GADTs.\n"
license-name: BSD3

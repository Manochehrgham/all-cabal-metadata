homepage: https://github.com/mstksg/backprop
changelog-type: markdown
hash: ab1a219d883c2573ac7b99256bc3b21b463750b4452fd77dd5ffd8197de0f274
test-bench-deps:
  mwc-random: -any
  type-combinators: -any
  base: -any
  time: -any
  criterion: -any
  bifunctors: -any
  mnist-idx: -any
  transformers: -any
  deepseq: -any
  generics-sop: -any
  hmatrix: ! '>=0.18'
  backprop: -any
  vector: -any
  directory: -any
maintainer: justin@jle.im
synopsis: Heterogeneous, type-safe automatic backpropagation in Haskell
changelog: ! "Changelog\n=========\n\nVersion 0.0.2.0\n---------------\n\n<https://github.com/mstksg/uncertain/releases/tag/v0.0.2.0>\n\n*
  \  Added optimized numeric `Op`s, and re-write `Num`/`Fractional`/`Floating`\n    instances
  in terms of them.\n\n*   Removed all traces of `Summer`/`Unity` from the library,
  eliminating a\n    whole swath of \"explicit-Summer\"/\"explicit-Unity\" versions
  of functions.\n    As a consequence, the library now only works with `Num` instances.
  \ The\n    API, however, is now much more simple.\n\n*   Benchmark suite added for
  MNIST example.\n\nVersion 0.0.1.0\n---------------\n\n<https://github.com/mstksg/uncertain/releases/tag/v0.0.1.0>\n\n*
  \  Initial pre-release, as a request for comments.  API is in a usable form\n    and
  everything is fully documented, but there are definitely some things\n    left to
  be done. (See [README.md][readme-0.0.1.0])\n\n    [readme-0.0.1.0]: https://github.com/mstksg/backprop/tree/v0.0.1.0#readme\n\n"
basic-deps:
  microlens-th: -any
  mwc-random: -any
  reflection: -any
  type-combinators: -any
  split: -any
  base: ! '>=4.7 && <5'
  time: -any
  ad: -any
  tagged: -any
  singletons: -any
  bifunctors: -any
  mtl: -any
  mnist-idx: -any
  transformers-base: -any
  transformers: -any
  deepseq: -any
  generics-sop: -any
  hmatrix: ! '>=0.18'
  backprop: -any
  microlens-mtl: -any
  microlens: -any
  primitive: -any
  profunctors: -any
  vector: -any
all-versions:
- '0.0.1.0'
- '0.0.2.0'
author: Justin Le
latest: '0.0.2.0'
description-type: markdown
description: ! "backprop\n========\n\n[![backprop on Hackage](https://img.shields.io/hackage/v/backprop.svg?maxAge=2592000)](https://hackage.haskell.org/package/backprop)\n[![Build
  Status](https://travis-ci.org/mstksg/backprop.svg?branch=master)](https://travis-ci.org/mstksg/backprop)\n\n[**Literate
  Haskell Tutorial/Demo on MNIST data set**][mnist-lhs] (and [PDF\nrendering][mnist-pdf])\n\nAutomatic
  *heterogeneous* back-propagation that can be used either *implicitly*\n(in the style
  of the [ad][] library) or using *explicit* graphs built in\nmonadic style.  Implements
  reverse-mode automatic differentiation.  Differs\nfrom [ad][] by offering full heterogeneity
  -- each intermediate step and the\nresulting value can have different types.  Mostly
  intended for usage with\ntensor manipulation libraries to implement automatic back-propagation
  for\ngradient descent and other optimization techniques.\n\n[ad]: http://hackage.haskell.org/package/ad\n\nCurrently
  up on [hackage][] (with 100% documentation coverage), but more\nup-to-date documentation
  is currently rendered [on github pages][docs]!\n\n[hackage]: http://hackage.haskell.org/package/backprop\n[docs]:
  https://mstksg.github.io/backprop\n\nAt the moment this project is in **pre-alpha**
  (*v0.0.1.0*), and is\npublished/put up on Hackage as a call for comments and thoughts.
  \ It has 100%\ndocumentation coverage at the moment.  Performance was not yet a
  priority\nbefore this, but will be from now on.  (Previously, highest priority was\nAPI/usability).
  See [the todos section][todos] for more information on what's\nmissing, and how
  one would be able to contribute!\n\n[todos]: https://github.com/mstksg/backprop#todo\n\nMNIST
  Digit Classifier Example\n------------------------------\n\nTutorial and example
  on training on the MNIST data set [available here as a\nliterate haskell file][mnist-lhs],
  or [rendered here as a PDF][mnist-pdf]!\n**Read this first!**\n\n[mnist-lhs]: https://github.com/mstksg/backprop/blob/master/samples/MNIST.lhs\n[mnist-pdf]:
  https://github.com/mstksg/backprop/blob/master/renders/MNIST.pdf\n\n\nBrief example\n-------------\n\nThe
  quick example below describes the running of a neural network with one\nhidden layer
  to calculate its squared error with respect to target `targ`,\nwhich is parameterized
  by two weight matrices and two bias vectors.\nVector/matrix types are from the *hmatrix*
  package.\n\n~~~haskell\nlogistic :: Floating a => a -> a\nlogistic x = 1 / (1 +
  exp (-x))\n\nmatVec\n    :: (KnownNat m, KnownNat n)\n    => Op '[ L m n, R n ]
  (R m)\n\nneuralNetImplicit\n      :: (KnownNat m, KnownNat n, KnownNat o)\n      =>
  R m\n      -> BPOpI s '[ L n m, R n, L o n, R o ] (R o)\nneuralNetImplicit inp =
  \\(w1 :< b1 :< w2 :< b2 :< Ø) ->\n    let z = logistic (liftB2 matVec w1 x + b1)\n
  \   in  logistic (liftB2 matVec w2 z + b2)\n  where\n    x = constRef inp\n\nneuralNetExplicit\n
  \     :: (KnownNat m, KnownNat n, KnownNat o)\n      => R m\n      -> BPOp s '[
  L n m, R n, L o n, R o ] (R o)\nneuralNetExplicit inp = withInps $ \\(w1 :< b1 :<
  w2 :< b2 :< Ø) -> do\n    y1  <- matVec ~$ (w1 :< x1 :< Ø)\n    let x2 = logistic
  (y1 + b1)\n    y2  <- matVec ~$ (w2 :< x2 :< Ø)\n    return $ logistic (y2 + b2)\n
  \ where\n    x1 = constVar inp\n~~~\n\nNow `neuralNetExplicit` and `neuralNetImplicit`
  can be \"run\" with the input\nvectors and parameters (a `L n m`, `R n`, `L o n`,
  and `R o`) and calculate the\noutput of the neural net.\n\n~~~haskell\nrunNet\n
  \   :: (KnownNat m, KnownNat n, KnownNat o)\n    => R m\n    -> Tuple '[ L n m,
  R n, L o n, R o ]\n    -> R o\nrunNet inp = evalBPOp (neuralNetExplicit inp)\n~~~\n\nBut,
  in defining `neuralNet`, we also generated a graph that *backprop* can\nuse to do
  back-propagation, too!\n\n~~~haskell\ndot :: KnownNat n\n    => Op '[ R n  , R n
  ] Double\n\nnetGrad\n    :: forall m n o. (KnownNat m, KnownNat n, KnownNat o)\n
  \   => R m\n    -> R o\n    -> Tuple '[ L n m, R n, L o n, R o ]\n    -> Tuple '[
  L n m, R n, L o n, R o ]\nnetGrad inp targ params = gradBPOp opError params\n  where\n
  \   -- calculate squared error, in *explicit* style\n    opError :: BPOp s '[ L
  n m, R n, L o n, R o ] Double\n    opError = do\n        res <- neuralNetExplicit
  inp\n        err <- bindRef (res - t)\n        dot ~$ (err :< err :< Ø)\n      where\n
  \       t = constRef targ\n~~~\n\nThe result is the gradient of the input tuple's
  components, with respect\nto the `Double` result of `opError` (the squared error).
  \ We can then use\nthis gradient to do gradient descent.\n\nFor a more fleshed out
  example, see the [MNIST tutorial][mnist-lhs] (also\n[rendered as a pdf][mnist-pdf])\n\nBenchmarks\n----------\n\nThe
  current version isn't optimized, but here are some basic benchmarks\ncomparing the
  library's automatic differentiation process to \"manual\"\ndifferentiation by hand.
  \ When using the [MNIST tutorial][bench] as an\nexample:\n\n[bench]: https://github.com/mstksg/backprop/blob/master/bench/MNISTBench.hs\n\n![benchmarks](http://i.imgur.com/xIZbhHa.png)\n\nCalculating
  the gradient using *backprop* and calculating it by hand (by manual\nsymbolic differentiation)
  are within an order of magnitude of each-other,\ntime-wise.  Using the *backprop*
  library takes about *6.5x* as long\nin this case.\n\nHowever, a full *update* step
  (calculate the gradient and update the neural\nnet) adds a lot of constant costs,
  so for a full training step, the *backprop*\nlibrary takes only *2.7x* as long as
  manual symbolic differentation.\n\nThis means using this library only slows down
  your program by a factor of\nabout 2.5x, compared to using only *hmatrix*.\n\nIt's
  still definitely not ideal that more than half of the computation time is\noverhead
  from the library, but this is just where we stand at the moment.\nOptimization is
  just now starting!\n\nNote that at the moment, simply running the network is only
  slightly slower\nwhen using *backprop*.\n\nTodo\n----\n\n1.  Profiling, to gauge
  where the overhead comes from (compared to \"manual\"\n    back-propagation) and
  how to bring it down.\n\n2.  Some simple performance and API tweaks that are probably
  possible now and\n    would clearly benefit: (if you want to contribute)\n\n    a.
  \ ~~Providing optimized `Num`/`Fractional`/`Floating` instances for `BVal`\n        by
  supplying known gradients directly instead of relying on *ad*.~~\n        (Now finished,
  since [b3898ae][optnum])\n\n[optnum]: https://github.com/mstksg/backprop/commit/b3898ae676b8048e03709fb5d3d38a6fedb48e1e\n\n
  \   b.  Switch from `ST s` to `IO`, and use `unsafePerformIO` to automatically\n
  \       bind `BVal`s (like *ad* does) when using `liftB`.  This might remove\n        some
  overhead during graph building, and, from an API standpoint,\n        remove the
  need for explicit binding.\n\n    c.  Switch from `STRef`s/`IORef`s to `Array`.
  \ (This one I'm unclear if it\n        would help any)\n\n3.  Benchmark against
  competing back-propagation libraries like *ad*, and\n    auto-differentiating tensor
  libraries like *[grenade][]*\n\n[grenade]: https://github.com/HuwCampbell/grenade\n\n4.
  \ Explore opportunities for parallelization.  There are some naive ways of\n    directly
  parallelizing right now, but potential overhead should be\n    investigated.\n\n5.
  \ Some open questions:\n\n    a.  Is it possible to offer pattern matching on sum
  types/with different\n        constructors for implicit-graph backprop?  It's possible
  for\n        explicit-graph versions already, with `choicesVar`, but not yet with\n
  \       the implicit-graph interface.  Could be similar to an \"Applicative vs.\n
  \       Monad\" issue where you can only have pre-determined fixed computation\n
  \       paths when using `Applicative`, but I'm not sure.  Still, it would be\n
  \       nice, because if this was possible, we could possibly do away with\n        explicit-graph
  mode completely.\n\n    b.  Though we already have sum type support with explicit-graph
  mode, we\n        can't support GADTs yet.  It'd be nice to see if this is possible,\n
  \       because a lot of dependently typed neural network stuff is made much\n        simpler
  with GADTs.\n"
license-name: BSD3

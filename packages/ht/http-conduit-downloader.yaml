changelog-type: ''
hash: 8afbf0bc39d8a34a7aa698383f39fa29f50be2e6aaed6a0d400b3c9951c31014
synopsis: HTTP downloader tailored for web-crawler needs.
changelog: ''
all-versions:
- '1.0.0'
- '1.0.1'
- '1.0.2'
- '1.0.3'
- '1.0.4'
- '1.0.5'
- '1.0.6'
- '1.0.7'
- '1.0.8'
- '1.0.9'
- '1.0.10'
- '1.0.11'
- '1.0.12'
- '1.0.13'
- '1.0.14'
- '1.0.15'
- '1.0.16'
- '1.0.17'
- '1.0.18'
- '1.0.19'
- '1.0.20'
- '1.0.21'
- '1.0.22'
- '1.0.23'
- '1.0.24'
- '1.0.25'
latest: '1.0.25'
description-type: haddock
description: ! 'HTTP/HTTPS downloader built on top of @http-conduit@

  and used in <https://bazqux.com> crawler.


  * Handles all possible http-conduit exceptions and returns

  human readable error messages.


  * Handles some web server bugs (returning ''deflate'' data instead of ''gzip'',

  invalid ''gzip'' encoding).


  * Uses OpenSSL instead of ''tls'' package (since ''tls'' doesn''t handle all sites).


  * Ignores invalid SSL sertificates.


  * Receives data in 32k chunks internally to reduce memory fragmentation

  on many parallel downloads.


  * Download timeout.


  * Total download size limit.


  * Returns HTTP headers for subsequent redownloads and handles

  ''Not modified'' results.


  * Can be used with external DNS resolver (e.g. concurrent-dns-cache).'

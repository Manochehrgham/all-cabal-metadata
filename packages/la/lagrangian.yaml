changelog-type: ''
hash: 3a9901675510c9c1b4048e43e8105241bbfc376ff4c71a56f747147b64aa2bde
synopsis: Solve Lagrange multiplier problems
changelog: ''
all-versions:
- '0.1.0.0'
- '0.2.0.0'
- '0.2.0.1'
- '0.2.0.2'
- '0.3.0.0'
- '0.3.0.1'
- '0.4.0.0'
- '0.4.0.1'
- '0.5.0.0'
- '0.6.0.0'
- '0.6.0.1'
latest: '0.6.0.1'
description-type: haddock
description: ! 'Numerically solve convex Lagrange multiplier problems with conjugate
  gradient descent.


  For some background on the method of Lagrange multipliers checkout the wikipedia
  page

  <http://en.wikipedia.org/wiki/Lagrange_multiplier>


  Here is an example from the Wikipedia page on Lagrange multipliers

  Maximize f(x, y) = x + y, subject to the constraint x^2 + y^2 = 1


  @

  \> maximize 0.00001 (\\[x, y] -> x + y) [(\\[x, y] -> x^2 + y^2) \<=\> 1] 2

  Right ([0.707,0.707], [-0.707])

  @


  For more information look here: <http://en.wikipedia.org/wiki/Lagrange_multiplier#Example_1>


  For example, to find the maximum entropy with the constraint that the probabilities
  sum

  to one.


  @

  \> maximize 0.00001 (negate . sum . map (\\x -> x * log x)) [sum \<=\> 1] 3

  Right ([0.33, 0.33, 0.33], [-0.09])

  @


  The first elements of the result pair are the arguments for the

  objective function at the maximum. The second elements are the Lagrange multipliers.

'

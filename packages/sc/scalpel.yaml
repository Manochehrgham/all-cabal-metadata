homepage: https://github.com/fimad/scalpel
changelog-type: markdown
hash: 5b27f0fac90f1f162ed97346cf20f273e62bbc4f70134c55f2d744d629f45374
test-bench-deps:
  base: ! '>=4.7 && <5'
  text: -any
  criterion: ! '>=1.1'
  HUnit: -any
  scalpel: -any
  regex-tdfa: -any
  regex-base: -any
  tagsoup: -any
maintainer: willcoster@gmail.com
synopsis: A high level web scraping library for Haskell.
changelog: ! "# Change Log\n\n## HEAD\n\n## 0.3.0.1\n\n- Make tag and attribute matching
  case-insensitive.\n\n## 0.3.0\n\n- Added benchmarks and many optimizations.\n- The
  `select` method is removed from the public API.\n- Many methods now have a constraint
  that the string type parametrizing\n  TagSoup's tag type now must be order-able.\n-
  Added `scrapeUrlWithConfig` that will hopefully put an end to multiplying\n  `scrapeUrlWith*`
  methods.\n- The default behaviour of the `scrapeUrl*` methods is to attempt to infer
  the\n  character encoding from the `Content-Type` header.\n\n## 0.2.1.1\n\n- Cleanup
  stale instance references in documentation of TagName and\n  AttributeName.\n\n##
  0.2.1\n\n- Made Scraper an instance of MonadPlus.\n\n## 0.2.0.1\n\n- Fixed examples
  in documentation and added an examples folder for ready to\n  compile examples.
  Added travis tests to ensures that examples remain\n  compilable.\n\n## 0.2.0\n\n-
  Removed the StringLike parameter from the Selector, Selectable,\n  AttributePredicate,
  AttributeName, and TagName types. Instead they are now\n  agnostic to the underlying
  string type, and are only constructable with\n  Strings and the Any type.\n\n##
  0.1.3.1\n\n- Tighten dependencies and drop download-curl all together.\n\n## 0.1.3\n\n-
  Add the html and html scraper primitives for extracting raw HTML.\n\n## 0.1.2\n\n-
  Make scrapeURL follow redirects by default.\n- Expose a new function scrapeURLWithOpts
  that takes a list of curl options.\n- Fix bug (#2) where image tags that do not
  have a trailing \"/\" are not\n  selectable.\n\n## 0.1.1\n\n- Tighten dependencies
  on download-curl.\n\n## 0.1.0\n\n- First version!\n"
basic-deps:
  bytestring: -any
  base: ! '>=4.6 && <5'
  text: -any
  curl: ! '>=1.3.4'
  data-default: -any
  containers: -any
  regex-tdfa: -any
  regex-base: -any
  tagsoup: ! '>=0.12.2'
all-versions:
- '0.1.0'
- '0.1.1'
- '0.1.2'
- '0.1.3'
- '0.1.3.1'
- '0.2.0'
- '0.2.0.1'
- '0.2.1'
- '0.2.1.1'
- '0.3.0'
- '0.3.0.1'
author: Will Coster
latest: '0.3.0.1'
description-type: markdown
description: ! "Scalpel [![Build Status](https://travis-ci.org/fimad/scalpel.svg?branch=master)](https://travis-ci.org/fimad/scalpel)
  [![Hackage](https://img.shields.io/hackage/v/scalpel.svg)](https://hackage.haskell.org/package/scalpel)\n=======\n\nScalpel
  is a web scraping library inspired by libraries like\n[Parsec](http://hackage.haskell.org/package/parsec-3.1.7/docs/Text-Parsec.html)\nand
  Perl's [Web::Scraper](http://search.cpan.org/~miyagawa/Web-Scraper-0.38/).\nScalpel
  builds on top of [TagSoup](http://hackage.haskell.org/package/tagsoup)\nto provide
  a declarative and monadic interface.\n\nThere are two general mechanisms provided
  by this library that are used to build\nweb scrapers: Selectors and Scrapers.\n\nSelectors\n---------\n\nSelectors
  describe a location within an HTML DOM tree. The simplest selector,\nthat can be
  written is a simple string value. For example, the selector\n`\"div\"` matches every
  single div node in a DOM. Selectors can be combined\nusing tag combinators. The
  `//` operator to define nested relationships within a\nDOM tree. For example, the
  selector `\"div\" // \"a\"` matches all anchor tags\nnested arbitrarily deep within
  a div tag.\n\nIn addition to describing the nested relationships between tags, selectors
  can\nalso include predicates on the attributes of a tag. The `@:` operator creates
  a\nselector that matches a tag based on the name and various conditions on the\ntag's
  attributes. An attribute predicate is just a function that takes an\nattribute and
  returns a boolean indicating if the attribute matches a criteria.\nThere are several
  attribute operators that can be used to generate common\npredicates. The `@=` operator
  creates a predicate that matches the name and\nvalue of an attribute exactly. For
  example, the selector `\"div\" @: [\"id\" @=\n\"article\"]` matches div tags where
  the id attribute is equal to `\"article\"`.\n\nScrapers\n--------\n\nScrapers are
  values that are parameterized over a selector and produce a value\nfrom an HTML
  DOM tree. The `Scraper` type takes two type parameters. The first\nis the string
  like type that is used to store the text values within a DOM tree.\nAny string like
  type supported by `Text.StringLike` is valid. The second type\nis the type of value
  that the scraper produces.\n\nThere are several scraper primitives that take selectors
  and extract content\nfrom the DOM. Each primitive defined by this library comes
  in two variants:\nsingular and plural. The singular variants extract the first instance
  matching\nthe given selector, while the plural variants match every instance.\n\nExample\n-------\n\nComplete
  examples can be found in the\n[examples](https://github.com/fimad/scalpel/tree/master/examples)
  folder in the\nscalpel git repository.\n\nThe following is an example that demonstrates
  most of the features provided by\nthis library. Supposed you have the following
  hypothetical HTML located at\n`\"http://example.com/article.html\"` and you would
  like to extract a list of all\nof the comments.\n\n```html\n<html>\n  <body>\n    <div
  class='comments'>\n      <div class='comment container'>\n        <span class='comment
  author'>Sally</span>\n        <div class='comment text'>Woo hoo!</div>\n      </div>\n
  \     <div class='comment container'>\n        <span class='comment author'>Bill</span>\n
  \       <img class='comment image' src='http://example.com/cat.gif' />\n      </div>\n
  \     <div class='comment container'>\n        <span class='comment author'>Susan</span>\n
  \       <div class='comment text'>WTF!?!</div>\n      </div>\n    </div>\n  </body>\n</html>\n```\n\nThe
  following snippet defines a function, `allComments`, that will download\nthe web
  page, and extract all of the comments into a list:\n\n```haskell\ntype Author =
  String\n\ndata Comment\n    = TextComment Author String\n    | ImageComment Author
  URL\n    deriving (Show, Eq)\n\nallComments :: IO (Maybe [Comment])\nallComments
  = scrapeURL \"http://example.com/article.html\" comments\n   where\n       comments
  :: Scraper String [Comment]\n       comments = chroots (\"div\" @: [hasClass \"container\"])
  comment\n\n       comment :: Scraper String Comment\n       comment = textComment
  <|> imageComment\n\n       textComment :: Scraper String Comment\n       textComment
  = do\n           author      <- text $ \"span\" @: [hasClass \"author\"]\n           commentText
  <- text $ \"div\"  @: [hasClass \"text\"]\n           return $ TextComment author
  commentText\n\n       imageComment :: Scraper String Comment\n       imageComment
  = do\n           author   <- text       $ \"span\" @: [hasClass \"author\"]\n           imageURL
  <- attr \"src\" $ \"img\"  @: [hasClass \"image\"]\n           return $ ImageComment
  author imageURL\n```\n"
license-name: Apache-2.0
